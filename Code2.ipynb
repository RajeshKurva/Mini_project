{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "          ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
       " 0       NaN  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
       " 1  3.716080  129.422921  18630.057858     6.635246         NaN    592.885359   \n",
       " 2  8.099124  224.236259  19909.541732     9.275884         NaN    418.606213   \n",
       " 3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
       " 4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
       " \n",
       "    Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       " 0       10.379783        86.990970   2.963135           0  \n",
       " 1       15.180013        56.329076   4.500656           0  \n",
       " 2       16.868637        66.420093   3.055934           0  \n",
       " 3       18.436524       100.341674   4.628771           0  \n",
       " 4       11.558279        31.997993   4.075075           0  )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first load and inspect the contents of the uploaded file to understand its structure and determine the necessary preprocessing steps.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/workspaces/Mini_project/water_potability.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information and the first few rows of the dataset\n",
    "data.info(), data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sklearn\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph                 14.987790\n",
       "Hardness            0.000000\n",
       "Solids              0.000000\n",
       "Chloramines         0.000000\n",
       "Sulfate            23.840049\n",
       "Conductivity        0.000000\n",
       "Organic_carbon      0.000000\n",
       "Trihalomethanes     4.945055\n",
       "Turbidity           0.000000\n",
       "Potability          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentage of missing values in each column to decide the strategy\n",
    "missing_percentage = data.isnull().mean() * 100\n",
    "\n",
    "# Display the percentage of missing values\n",
    "missing_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15298/4011163246.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['ph'].fillna(data['ph'].median(), inplace=True)\n",
      "/tmp/ipykernel_15298/4011163246.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Sulfate'].fillna(data['Sulfate'].median(), inplace=True)\n",
      "/tmp/ipykernel_15298/4011163246.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Trihalomethanes'].fillna(data['Trihalomethanes'].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ph                 0\n",
       "Hardness           0\n",
       "Solids             0\n",
       "Chloramines        0\n",
       "Sulfate            0\n",
       "Conductivity       0\n",
       "Organic_carbon     0\n",
       "Trihalomethanes    0\n",
       "Turbidity          0\n",
       "Potability         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute missing values using the median of each column\n",
    "data['ph'].fillna(data['ph'].median(), inplace=True)\n",
    "data['Sulfate'].fillna(data['Sulfate'].median(), inplace=True)\n",
    "data['Trihalomethanes'].fillna(data['Trihalomethanes'].median(), inplace=True)\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6585\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.89      0.76       400\n",
      "           1       0.63      0.30      0.41       256\n",
      "\n",
      "    accuracy                           0.66       656\n",
      "   macro avg       0.65      0.59      0.58       656\n",
      "weighted avg       0.65      0.66      0.62       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Separate features and target variable\n",
    "X = data.drop('Potability', axis=1)\n",
    "y = data['Potability']\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Initialize a StandardScaler for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 4: Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Choose a classification model (Random Forest in this case)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 8: Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.4s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.4s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.6s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.6s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.6s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.2s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.6s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.3s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.9s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.7s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.5s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.6s\n",
      "[CV] END bootstrap=True, max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   1.2s\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Improved Accuracy: 0.6723\n",
      "Classification Report for the Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77       400\n",
      "           1       0.67      0.32      0.43       256\n",
      "\n",
      "    accuracy                           0.67       656\n",
      "   macro avg       0.67      0.61      0.60       656\n",
      "weighted avg       0.67      0.67      0.64       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Set up the reduced hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Fewer values\n",
    "    'max_depth': [10, 20],  # Reduced options\n",
    "    'min_samples_split': [2, 5],  # Only critical values\n",
    "    'min_samples_leaf': [1, 2],  # Limited to 2 values\n",
    "    'bootstrap': [True]  # Focus on one option to reduce complexity\n",
    "}\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 3: Set up GridSearchCV with 3-fold cross-validation (reduced from 5)\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Step 4: Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Get the best parameters and performance\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "# Step 6: Print results\n",
    "from sklearn.metrics import classification_report\n",
    "report_best = classification_report(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Improved Accuracy: {accuracy_best:.4f}\")\n",
    "print(\"Classification Report for the Best Model:\")\n",
    "print(report_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
